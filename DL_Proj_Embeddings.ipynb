{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsbombpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.14.0)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: fastparquet in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2024.11.0)\n",
      "Requirement already satisfied: pyarrow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (18.1.0)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from statsbombpy) (2.1.4)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from statsbombpy) (2.32.3)\n",
      "Requirement already satisfied: requests-cache in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from statsbombpy) (1.2.1)\n",
      "Requirement already satisfied: inflect in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from statsbombpy) (7.5.0)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from statsbombpy) (1.4.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastparquet) (2.9.1)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastparquet) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->statsbombpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->statsbombpy) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->statsbombpy) (2024.2)\n",
      "Requirement already satisfied: more_itertools>=8.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from inflect->statsbombpy) (10.5.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from inflect->statsbombpy) (4.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->statsbombpy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->statsbombpy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->statsbombpy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->statsbombpy) (2024.12.14)\n",
      "Requirement already satisfied: attrs>=21.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests-cache->statsbombpy) (24.3.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests-cache->statsbombpy) (24.1.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests-cache->statsbombpy) (4.3.6)\n",
      "Requirement already satisfied: url-normalize>=1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests-cache->statsbombpy) (1.4.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cattrs>=22.2->requests-cache->statsbombpy) (1.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->statsbombpy) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install statsbombpy transformers fastparquet pyarrow\n",
    "from statsbombpy import sb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/statsbombpy/api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m events \u001b[38;5;241m=\u001b[39m \u001b[43msb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompetition_events\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcountry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEngland\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdivision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPremier League\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2015/2016\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Sort the events DataFrame by minute and second\u001b[39;00m\n\u001b[1;32m      8\u001b[0m events \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminute\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecond\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/statsbombpy/sb.py:140\u001b[0m, in \u001b[0;36mcompetition_events\u001b[0;34m(country, division, season, gender, split, filters, fmt, creds, include_360_metrics)\u001b[0m\n\u001b[1;32m    133\u001b[0m events_call \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    134\u001b[0m     events,\n\u001b[1;32m    135\u001b[0m     fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    136\u001b[0m     creds\u001b[38;5;241m=\u001b[39mcreds,\n\u001b[1;32m    137\u001b[0m     include_360_metrics\u001b[38;5;241m=\u001b[39minclude_360_metrics,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(MAX_CONCURRENCY) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m--> 140\u001b[0m     matches_events \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevents_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompetition_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseason_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     matches_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m events: filter_and_group_events(\n\u001b[1;32m    146\u001b[0m             events, filters, fmt, fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         ),\n\u001b[1;32m    148\u001b[0m         matches_events,\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    151\u001b[0m competition_events \u001b[38;5;241m=\u001b[39m reduce_events(matches_events, fmt)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "events = sb.competition_events(\n",
    "    country=\"England\",\n",
    "    division= \"Premier League\",\n",
    "    season=\"2015/2016\",\n",
    "    gender=\"male\"\n",
    ")\n",
    "# Sort the events DataFrame by minute and second\n",
    "events = events.sort_values(by=[\"minute\", \"second\"])\n",
    "\n",
    "# Display the first few rows of the filtered and sorted DataFrame\n",
    "print(events.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def extract_template_fields(template):\n",
    "    \"\"\"Extract field names from a template string using regex.\"\"\"\n",
    "    return set(re.findall(r'\\{(\\w+)\\}', template))\n",
    "\n",
    "def is_valid_value(value):\n",
    "    \"\"\"Check if a value is valid (not None, nan, or empty).\"\"\"\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return len(value) > 0\n",
    "    return pd.notna(value)\n",
    "\n",
    "def format_value(value):\n",
    "    \"\"\"Format values for display, handling arrays and other types.\"\"\"\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return str(value)\n",
    "    return str(value)\n",
    "\n",
    "def get_player_info(row):\n",
    "    \"\"\"Extract player ID and name from row.\"\"\"\n",
    "    try:\n",
    "        if isinstance(row.get('player'), dict):\n",
    "            player_id = str(row['player'].get('id', 'unknown'))\n",
    "            player_name = row['player'].get('name', 'unknown')\n",
    "        else:\n",
    "            player_id = str(row.get('player_id', 'unknown'))\n",
    "            player_name = row.get('player', 'unknown')\n",
    "        return player_id, player_name\n",
    "    except:\n",
    "        return 'unknown', 'unknown'\n",
    "\n",
    "def generate_natural_language(row, event_type):\n",
    "    \"\"\"Generate natural language description for an event with proper error handling.\"\"\"\n",
    "    template = event_templates.get(event_type, \"An event of type {type} occurred.\")\n",
    "    required_fields = extract_template_fields(template)\n",
    "    \n",
    "    values = {}\n",
    "    for field in required_fields:\n",
    "        try:\n",
    "            value = row.get(field)\n",
    "            if is_valid_value(value):\n",
    "                values[field] = format_value(value)\n",
    "            else:\n",
    "                values[field] = \"unknown\"\n",
    "        except (KeyError, AttributeError):\n",
    "            values[field] = \"unknown\"\n",
    "    \n",
    "    try:\n",
    "        # Add timestamp to the description to help verify chronological order\n",
    "        timestamp = row.get('timestamp', 'unknown')\n",
    "        minute = row.get('minute', 'unknown')\n",
    "        second = row.get('second', 'unknown')\n",
    "        time_prefix = f\"[{minute}:{second} - {timestamp}] \"\n",
    "        return time_prefix + template.format_map(values)\n",
    "    except KeyError as e:\n",
    "        return f\"Error generating description for {event_type}: missing field {str(e)}\"\n",
    "\n",
    "def process_events(events_df, output_dir='event_descriptions'):\n",
    "    \"\"\"Process events DataFrame and write descriptions to files in chronological order.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    open_files = {}\n",
    "    try:\n",
    "        for _, row in events_df.iterrows():\n",
    "            if row.get('position') == \"Goalkeeper\":\n",
    "                continue\n",
    "                \n",
    "            event_type = row.get('type')\n",
    "            if event_type not in event_templates:\n",
    "                continue\n",
    "                \n",
    "            player_id, player_name = get_player_info(row)\n",
    "            match_id = str(row.get('match_id', 'unknown'))\n",
    "            \n",
    "            filename = f\"{player_name.replace(' ', '_')}_{player_id}_{match_id}.txt\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            \n",
    "            if filename not in open_files:\n",
    "                open_files[filename] = open(filepath, 'w')\n",
    "            \n",
    "            description = generate_natural_language(row, event_type)\n",
    "            open_files[filename].write(description + '\\n')\n",
    "            open_files[filename].flush()\n",
    "    \n",
    "    finally:\n",
    "        for file in open_files.values():\n",
    "            file.close()\n",
    "\n",
    "event_templates = {\n",
    "    \"Pass\": (\n",
    "        \"A {pass_type} pass with {pass_body_part} was made from [x, y] coordinates: {location} \"\n",
    "        \"to [x, y] coordinates: {pass_end_location}. The pass was {pass_outcome}. \"\n",
    "        \"Pass length: {pass_length} meters. Counterpress: {counterpress}. \"\n",
    "        \"Under pressure: {under_pressure}.\"\n",
    "    ),\n",
    "    \"Shot\": (\n",
    "        \"A {shot_type} shot with {shot_body_part} was taken at [x, y] coordinates: {location}. \"\n",
    "        \"The shot was a {shot_outcome}, with xG: {shot_statsbomb_xg}. \"\n",
    "        \"First time: {shot_first_time}, One-on-one: {shot_one_on_one}. \"\n",
    "        \"Under pressure: {under_pressure}.\"\n",
    "    ),\n",
    "    \"Block\": (\n",
    "        \"A block occurred at [x, y] coordinates: {location}. Counterpress: {counterpress}. \"\n",
    "        \"Play pattern: {play_pattern}. Out: {out}.\"\n",
    "    ),\n",
    "    \"Ball Recovery\": (\n",
    "        \"A ball recovery happened at [x, y] coordinates: {location}. Recovery failure: {ball_recovery_recovery_failure}. \"\n",
    "        \"Play pattern: {play_pattern}. Under pressure: {under_pressure}.\"\n",
    "    ),\n",
    "    \"Miscontrol\": (\n",
    "        \"A miscontrol occurred at [x, y] coordinates: {location}. Out: {out}. \"\n",
    "        \"Play pattern: {play_pattern}. Under pressure: {under_pressure}.\"\n",
    "    ),\n",
    "    \"Clearance\": (\n",
    "        \"A clearance occurred at [x, y] coordinates: {location} with {clearance_body_part}. \"\n",
    "        \"Aerial won: {clearance_aerial_won}. Out: {out}. \"\n",
    "        \"Play pattern: {play_pattern}. Under pressure: {under_pressure}.\"\n",
    "    ),\n",
    "    \"Dribbled Past\": (\n",
    "        \"A player was dribbled past at [x, y] coordinates: {location}. Counterpress: {counterpress}. \"\n",
    "        \"Play pattern: {play_pattern}.\"\n",
    "    ),\n",
    "    \"Dribble\": (\n",
    "        \"A dribble was attempted at [x, y] coordinates: {location}. Outcome: {dribble_outcome}, \"\n",
    "        \"Overrun: {dribble_overrun}. Play pattern: {play_pattern}. \"\n",
    "        \"Under pressure: {under_pressure}.\"\n",
    "    ),\n",
    "    \"Interception\": (\n",
    "        \"An interception occurred at [x, y] coordinates: {location}. Outcome: {interception_outcome}. \"\n",
    "        \"Counterpress: {counterpress}. Play pattern: {play_pattern}.\"\n",
    "    ),\n",
    "    \"Foul Committed\": (\n",
    "        \"A foul was committed at [x, y] coordinates: {location}. Type: {foul_committed_type}, \"\n",
    "        \"Card: {foul_committed_card}, Offensive: {foul_committed_offensive}, \"\n",
    "        \"Penalty: {foul_committed_penalty}. Counterpress: {counterpress}. \"\n",
    "        \"Play pattern: {play_pattern}.\"\n",
    "    ),\n",
    "    \"Foul Won\": (\n",
    "        \"A foul was won at [x, y] coordinates: {location}. Defensive: {foul_won_defensive}, \"\n",
    "        \"Penalty: {foul_won_penalty}. Play pattern: {play_pattern}. \"\n",
    "        \"Under pressure: {under_pressure}.\"\n",
    "    ),\n",
    "    \"Shield\": (\n",
    "        \"A shield occurred at {location}. Play pattern: {play_pattern}.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    process_events(events)\n",
    "    print(\"Processing complete. Check the 'event_descriptions' directory for output files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.47.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"\n",
    "    Configure the appropriate device (GPU/CPU) for processing.\n",
    "    Returns the device and a boolean indicating if CUDA is available.\n",
    "    \"\"\"\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "def read_and_embed_player_matches(event_descriptions_dir, model_name='jinaai/jina-embeddings-v2-small-en', batch_size=32):\n",
    "    \"\"\"\n",
    "    Read player match files, generate embeddings, and create a structured DataFrame.\n",
    "    Includes performance optimizations and player filtering.\n",
    "    \"\"\"\n",
    "    device = setup_device()\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = model.to(device)  # Move model to GPU if available\n",
    "    model.eval()\n",
    "    \n",
    "    # First pass: count matches per player\n",
    "    player_match_counts = Counter()\n",
    "    files = [f for f in os.listdir(event_descriptions_dir) if f.endswith('.txt')]\n",
    "    \n",
    "    for filename in files:\n",
    "        player_name = filename.split('_')[0]\n",
    "        player_match_counts[player_name] += 1\n",
    "    \n",
    "    # Filter players with fewer than 10 matches\n",
    "    qualified_players = {player for player, count in player_match_counts.items() \n",
    "                        if count >= 10}\n",
    "    \n",
    "    # Filter files for qualified players\n",
    "    qualified_files = [f for f in files if f.split('_')[0] in qualified_players]\n",
    "    print(f\"Processing {len(qualified_files)} files from {len(qualified_players)} qualified players\")\n",
    "    \n",
    "    results = []\n",
    "    # Process in batches for GPU efficiency\n",
    "    for i in tqdm(range(0, len(qualified_files), batch_size), desc=\"Processing batches\"):\n",
    "        batch_files = qualified_files[i:i + batch_size]\n",
    "        batch_texts = []\n",
    "        batch_info = []\n",
    "        \n",
    "        # Read batch of files\n",
    "        for filename in batch_files:\n",
    "            player_name = filename.split('_')[0]\n",
    "            player_id = filename.split('_')[1]\n",
    "            match_id = filename.split('_')[2].replace('.txt', '')\n",
    "            \n",
    "            with open(os.path.join(event_descriptions_dir, filename), 'r') as f:\n",
    "                match_text = f.read()\n",
    "            \n",
    "            batch_texts.append(match_text)\n",
    "            batch_info.append({\n",
    "                'player_name': player_name,\n",
    "                'player_id': player_id,\n",
    "                'match_id': match_id,\n",
    "                'text': match_text\n",
    "            })\n",
    "        \n",
    "        # Generate embeddings for batch\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(batch_texts)\n",
    "            # Only move to CPU if embeddings are PyTorch tensors\n",
    "            if isinstance(embeddings, torch.Tensor):\n",
    "                embeddings = embeddings.cpu().numpy()  # Move back to CPU for storage\n",
    "\n",
    "        # Add results\n",
    "        for info, embedding in zip(batch_info, embeddings):\n",
    "            info['embedding'] = embedding\n",
    "            results.append(info)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add match count for each player\n",
    "    df['match_count'] = df['player_name'].map(player_match_counts)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_embeddings(df, output_path='player_match_embeddings.parquet'):\n",
    "    \"\"\"Save the DataFrame with embeddings efficiently.\"\"\"\n",
    "    # Convert embeddings to list format for storage\n",
    "    df['embedding'] = df['embedding'].apply(lambda x: x.tolist())\n",
    "    df.to_parquet(output_path)\n",
    "    print(f\"Embeddings saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_and_embed_player_matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Process files and generate embeddings\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m embeddings_df \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_embed_player_matches\u001b[49m(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_descriptions\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Adjust based on your GPU memory\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m     13\u001b[0m save_embeddings(embeddings_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_and_embed_player_matches' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage with performance monitoring\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process files and generate embeddings\n",
    "embeddings_df = read_and_embed_player_matches(\n",
    "    'event_descriptions',\n",
    "    batch_size=4  # Adjust based on your GPU memory\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_embeddings(embeddings_df)\n",
    "\n",
    "print(f\"Processing completed in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Total players: {embeddings_df['player_name'].nunique()}\")\n",
    "print(f\"Total matches: {len(embeddings_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastparquet) (2.1.4)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastparquet) (1.26.4)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastparquet) (2024.12.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Downloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m167.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cramjam-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m170.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, cramjam, fastparquet\n",
      "Successfully installed cramjam-2.9.1 fastparquet-2024.11.0 pyarrow-18.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA L4\n",
      "Processing 9233 files from 376 qualified players\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 9233/9233 [13:50<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to player_embeddings.parquet\n",
      "Processing completed in 834.44 seconds\n",
      "Total players: 376\n",
      "Total player embeddings: 376\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def mean_pooling(model_output):\n",
    "    \"\"\"\n",
    "    Perform mean pooling on the token embeddings to get a single document embedding.\n",
    "    The attention mask is used to avoid including padding tokens in the mean.\n",
    "    \"\"\"\n",
    "     # Get token embeddings from the model output\n",
    "    token_embeddings = model_output[0]  # Shape: (batch_size, seq_length, hidden_size)\n",
    "    \n",
    "    # Expand attention mask to match embedding dimensions\n",
    "    # Original mask shape: (batch_size, seq_length)\n",
    "    # New mask shape: (batch_size, seq_length, hidden_size)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    \n",
    "    # Multiply embeddings by mask to zero out padding tokens\n",
    "    # Then sum up all token embeddings for each sequence\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "    \n",
    "    # Count the non-padding tokens for each sequence\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "    \n",
    "    # Calculate mean by dividing sum of embeddings by number of non-padding tokens\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    # L2 normalize the embeddings\n",
    "    normalized_embeddings = F.normalize(mean_embeddings, p=2, dim=1)\n",
    "    \n",
    "    return normalized_embeddings\n",
    "    \n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"\n",
    "    Configure the appropriate device (GPU/CPU) for processing.\n",
    "    Returns the device and a boolean indicating if CUDA is available.\n",
    "    \"\"\"\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "        torch.cuda.set_per_process_memory_fraction(0.7)\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "def read_and_embed_player_matches(event_descriptions_dir, model_name='jinaai/jina-embeddings-v2-small-en', \n",
    "                                batch_size=32, max_seq_length=2048):\n",
    "    \"\"\"\n",
    "    Read player match files, generate embeddings, and create a structured DataFrame.\n",
    "    Returns a single embedding per document using mean pooling.\n",
    "    \"\"\"\n",
    "    device = setup_device()\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # First pass: count matches per player using consistent player name parsing\n",
    "    player_match_counts = Counter()\n",
    "    files = [f for f in os.listdir(event_descriptions_dir) if f.endswith('.txt')]\n",
    "    \n",
    "    for filename in files:\n",
    "        # Use the same parsing logic as in the main processing\n",
    "        player_id = filename.split('_')[-2]\n",
    "        player_match_counts[player_id] += 1\n",
    "    \n",
    "    qualified_players = {player for player, count in player_match_counts.items() \n",
    "                        if count >= 10}\n",
    "    \n",
    "    qualified_files = [f for f in files if f.split('_')[-2] in qualified_players]\n",
    "    print(f\"Processing {len(qualified_files)} files from {len(qualified_players)} qualified players\")\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        for i in tqdm(range(0, len(qualified_files), batch_size), desc=\"Processing batches\"):\n",
    "            batch_files = qualified_files[i:i + batch_size]\n",
    "            batch_texts = []\n",
    "            batch_info = []\n",
    "            \n",
    "            # Read batch of files\n",
    "            for filename in batch_files:\n",
    "                player_name = ' '.join(filename.split('_')[0:-2])\n",
    "\n",
    "                player_id = filename.split('_')[-2]\n",
    "                match_id = filename.split('_')[-1].replace('.txt', '')\n",
    "                \n",
    "                with open(os.path.join(event_descriptions_dir, filename), 'r') as f:\n",
    "                    match_text = f.read()\n",
    "                \n",
    "                batch_texts.append(match_text)\n",
    "                batch_info.append({\n",
    "                    'player_name': player_name,\n",
    "                    'player_id': player_id,\n",
    "                    'match_id': match_id,\n",
    "                    'text': match_text\n",
    "                })\n",
    "            \n",
    "            # Generate embeddings for batch with proper pooling\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast(enabled=True):\n",
    "                # If using the model's encode method (which handles pooling internally)\n",
    "                try:\n",
    "                    embeddings = model.encode(batch_texts, max_length=max_seq_length)\n",
    "                except AttributeError:\n",
    "                    # If encode isn't available, manually handle the forward pass and pooling\n",
    "                    inputs = model.tokenizer(batch_texts, \n",
    "                                          padding=True, \n",
    "                                          truncation=True, \n",
    "                                          max_length=max_seq_length, \n",
    "                                          return_tensors=\"pt\").to(device)\n",
    "                    outputs = model(**inputs)\n",
    "                    embeddings = mean_pooling(outputs).cpu().numpy()\n",
    "                else:\n",
    "                    if isinstance(embeddings, torch.Tensor):\n",
    "                        embeddings = embeddings.cpu().numpy()\n",
    "            \n",
    "            # Clear CUDA cache after each batch\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Add results\n",
    "            for info, embedding in zip(batch_info, embeddings):\n",
    "                info['embedding'] = embedding\n",
    "                results.append(info)\n",
    "            \n",
    "            # Explicitly delete batch data\n",
    "            del embeddings\n",
    "            del batch_texts\n",
    "            del batch_info\n",
    "    \n",
    "    finally:\n",
    "        # Clean up model resources\n",
    "        model.cpu()\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_embeddings(df, output_path='player_match_embeddings.parquet'):\n",
    "    \"\"\"Save the DataFrame with embeddings efficiently.\"\"\"\n",
    "    df['embedding'] = df['embedding'].apply(lambda x: x.tolist())\n",
    "    # Exclude 'match_count' from being saved to the DataFrame or Parquet\n",
    "    df = df.drop(columns=['match_count'], errors='ignore')\n",
    "    df.to_parquet(output_path)\n",
    "    print(f\"Embeddings saved to {output_path}\")\n",
    "\n",
    "def convert_to_player_embeddings(df):\n",
    "    \"\"\"\n",
    "    Convert the player_match embeddings DataFrame to a player-level embeddings DataFrame by averaging embeddings per player.\n",
    "    \"\"\"\n",
    "    # Convert embeddings from list to numpy array for easier handling\n",
    "    df['embedding'] = df['embedding'].apply(np.array)\n",
    "    \n",
    "    # Group by 'player_name' and average the embeddings for each player\n",
    "    player_embeddings_df = df.groupby('player_id').agg(\n",
    "    player_name=('player_name', 'first'),\n",
    "    embedding=('embedding', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "    \n",
    "    return player_embeddings_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read and embed player matches\n",
    "    embeddings_df = read_and_embed_player_matches(\n",
    "        'event_descriptions',\n",
    "        batch_size=1,\n",
    "        max_seq_length=8192\n",
    "    )\n",
    "    \n",
    "    # Convert to player embeddings (averaging across matches)\n",
    "    player_embeddings_df = convert_to_player_embeddings(embeddings_df)\n",
    "    \n",
    "    # Save player embeddings\n",
    "    save_embeddings(player_embeddings_df, 'player_embeddings.parquet')\n",
    "    \n",
    "    print(f\"Processing completed in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Total players: {player_embeddings_df['player_name'].nunique()}\")\n",
    "    print(f\"Total player embeddings: {len(player_embeddings_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    player_id       player_name  \\\n",
      "430    9641.0  Fraizer Campbell   \n",
      "431    9642.0       Alan Hutton   \n",
      "432    9649.0      Jack Colback   \n",
      "433    9930.0      Ryan Bennett   \n",
      "434    9958.0    Steven Caulker   \n",
      "\n",
      "                                             embedding  \n",
      "430  [-0.28325140476226807, -0.26176461577415466, 0...  \n",
      "431  [-0.21182601153850555, -0.21284833550453186, 0...  \n",
      "432  [-0.21501609683036804, -0.21565166115760803, 0...  \n",
      "433  [-0.19289526343345642, -0.2248104214668274, 0....  \n",
      "434  [-0.2230857014656067, -0.24024677276611328, 0....  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame from the saved Parquet file\n",
    "embeddings_df = pd.read_parquet('player_embeddings.parquet')\n",
    "\n",
    "# Display the first few rows to check the data\n",
    "print(embeddings_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/statsbombpy/api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings...\n",
      "Saving updated embeddings...\n",
      "\n",
      "Dataset statistics:\n",
      "Total players: 376\n",
      "Players with position data: 376\n",
      "Players with team data: 376\n",
      "\n",
      "Position distribution:\n",
      "position\n",
      "Center Forward               50\n",
      "Left Back                    36\n",
      "Right Back                   35\n",
      "Right Center Back            34\n",
      "Left Center Back             31\n",
      "Left Wing                    28\n",
      "Right Wing                   26\n",
      "Center Attacking Midfield    24\n",
      "Left Defensive Midfield      22\n",
      "Right Center Midfield        20\n",
      "Right Defensive Midfield     18\n",
      "Left Center Midfield         15\n",
      "Left Midfield                12\n",
      "Right Midfield               10\n",
      "Center Defensive Midfield     7\n",
      "Right Center Forward          5\n",
      "Left Center Forward           3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Team distribution:\n",
      "team\n",
      "Sunderland              22\n",
      "Norwich City            20\n",
      "Aston Villa             20\n",
      "Manchester City         20\n",
      "West Ham United         20\n",
      "Arsenal                 20\n",
      "Chelsea                 20\n",
      "Stoke City              20\n",
      "AFC Bournemouth         19\n",
      "Crystal Palace          19\n",
      "Manchester United       19\n",
      "Southampton             18\n",
      "West Bromwich Albion    18\n",
      "Liverpool               18\n",
      "Swansea City            18\n",
      "Newcastle United        18\n",
      "Watford                 18\n",
      "Everton                 17\n",
      "Leicester City          16\n",
      "Tottenham Hotspur       16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def add_player_metadata_from_events(embeddings_file, events_df, output_file):\n",
    "    \"\"\"\n",
    "    Load embeddings, add position and team data from events DataFrame, and save to a new file.\n",
    "    \n",
    "    Parameters:\n",
    "    embeddings_file (str): Path to existing player embeddings parquet file\n",
    "    events_df (pd.DataFrame): DataFrame containing events data with player, team, and position info\n",
    "    output_file (str): Path to save the new embeddings with metadata\n",
    "    \"\"\"\n",
    "    # Load existing embeddings\n",
    "    print(\"Loading existing embeddings...\")\n",
    "    embeddings_df = pd.read_parquet(embeddings_file)\n",
    "    \n",
    "    # Get position and team from events data\n",
    "    # Group by player and get the most frequent position and team\n",
    "    player_data = events_df.groupby('player').agg({\n",
    "        'position': lambda x: Counter(x).most_common(1)[0][0],\n",
    "        'team': lambda x: Counter(x).most_common(1)[0][0]\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns to match our existing data\n",
    "    player_data = player_data.rename(columns={'player': 'player_name'})\n",
    "    \n",
    "    # Merge position and team data with embeddings\n",
    "    # Using left merge to keep all players from embeddings\n",
    "    embeddings_df = embeddings_df.merge(player_data, on='player_name', how='left')\n",
    "    \n",
    "    # Save the updated DataFrame\n",
    "    print(\"Saving updated embeddings...\")\n",
    "    embeddings_df.to_parquet(output_file)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDataset statistics:\")\n",
    "    print(f\"Total players: {len(embeddings_df)}\")\n",
    "    print(f\"Players with position data: {embeddings_df['position'].notna().sum()}\")\n",
    "    print(f\"Players with team data: {embeddings_df['team'].notna().sum()}\")\n",
    "    \n",
    "    print(\"\\nPosition distribution:\")\n",
    "    print(embeddings_df['position'].value_counts())\n",
    "    \n",
    "    print(\"\\nTeam distribution:\")\n",
    "    print(embeddings_df['team'].value_counts())\n",
    "    \n",
    "    # Print players missing metadata\n",
    "    missing_metadata = embeddings_df[embeddings_df['position'].isna() | embeddings_df['team'].isna()]\n",
    "    if len(missing_metadata) > 0:\n",
    "        print(\"\\nPlayers missing metadata:\")\n",
    "        print(missing_metadata['player_name'].tolist())\n",
    "    \n",
    "    return embeddings_df\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "events = sb.competition_events(\n",
    "    country=\"England\",\n",
    "    division= \"Premier League\",\n",
    "    season=\"2015/2016\",\n",
    "    gender=\"male\"\n",
    ")\n",
    "\n",
    "# Assuming you have your events DataFrame loaded\n",
    "updated_df = add_player_metadata_from_events(\n",
    "    embeddings_file='player_embeddings.parquet',\n",
    "    events_df=events,  # Your events DataFrame\n",
    "    output_file='player_embeddings_with_metadata.parquet'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    player_id                         player_name  \\\n",
      "0     10457.0                     Patrick Bamford   \n",
      "1     10499.0  Ivo Daniel Ferreira Mendonca Pinto   \n",
      "2     10503.0                         Billy Jones   \n",
      "3     10779.0           Cameron Borthwick-Jackson   \n",
      "4     10783.0                      Lee Cattermole   \n",
      "..        ...                                 ...   \n",
      "371    9638.0                       Jack Grealish   \n",
      "372    9641.0                    Fraizer Campbell   \n",
      "373    9642.0                         Alan Hutton   \n",
      "374    9649.0                        Jack Colback   \n",
      "375    9930.0                        Ryan Bennett   \n",
      "\n",
      "                                             embedding  \\\n",
      "0    [-0.25265589356422424, -0.264048308134079, 0.1...   \n",
      "1    [-0.21007807552814484, -0.22289517521858215, 0...   \n",
      "2    [-0.20582975447177887, -0.21635812520980835, 0...   \n",
      "3    [-0.20873406529426575, -0.20124152302742004, 0...   \n",
      "4    [-0.20809024572372437, -0.2177758365869522, 0....   \n",
      "..                                                 ...   \n",
      "371  [-0.2139180600643158, -0.24781137704849243, 0....   \n",
      "372  [-0.2832649052143097, -0.2617599368095398, 0.1...   \n",
      "373  [-0.21182765066623688, -0.21284811198711395, 0...   \n",
      "374  [-0.21502076089382172, -0.21565116941928864, 0...   \n",
      "375  [-0.1928950399160385, -0.2248060554265976, 0.1...   \n",
      "\n",
      "                      position               team  \n",
      "0               Center Forward       Norwich City  \n",
      "1                   Right Back       Norwich City  \n",
      "2                   Right Back         Sunderland  \n",
      "3                    Left Back  Manchester United  \n",
      "4        Right Center Midfield         Sunderland  \n",
      "..                         ...                ...  \n",
      "371  Center Attacking Midfield        Aston Villa  \n",
      "372             Center Forward     Crystal Palace  \n",
      "373                 Right Back        Aston Villa  \n",
      "374    Left Defensive Midfield   Newcastle United  \n",
      "375          Right Center Back       Norwich City  \n",
      "\n",
      "[376 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(updated_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
